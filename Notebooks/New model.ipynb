{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-07T12:54:50.973687Z",
     "start_time": "2025-12-07T12:54:50.970039Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T12:54:51.023903Z",
     "start_time": "2025-12-07T12:54:51.018616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LeachateDataProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.rock_features = [\n",
    "            'EC_rock', 'Ph_rock', 'Corg_rock (%)', 'Ca_rock', 'K_rock',\n",
    "            'Mg_rock', 'Na_rock', 'SAR_rock', 'SiO2_rock', 'Al2O3_rock',\n",
    "            'Fe2O3_rock', 'TiO2_rock', 'MnO_rock', 'CaO_rock', 'MgO_rock',\n",
    "            'Na2O_rock', 'K2O_rock', 'SO3_rock', 'P2O5_rock'\n",
    "        ]\n",
    "        self.env_features = ['Type_event', 'Event_quantity', 'Acid', 'Temp', 'Timestep']\n",
    "        self.targets = [\n",
    "            'Volume_leachate', 'EC_leachate', 'Ph_leachate', 'Chloride_leachate',\n",
    "            'Carbonate_leachate', 'Sulfate_leachate', 'Nitrate_leachate',\n",
    "            'Phosphate_leachate', 'Ca_leachate', 'Fe_leachate', 'K_leachate',\n",
    "            'Mg_leachate', 'Mn_leachate', 'Na_leachate'\n",
    "        ]\n",
    "\n",
    "    def load_and_clean(self):\n",
    "        # Load data with semicolon delimiter\n",
    "        self.df = pd.read_csv(self.file_path, delimiter=';')\n",
    "\n",
    "        # 1. Replace European decimals (,) with dots (.)\n",
    "        # 2. Replace missing values '/' with NaN, then fill with 0 (assuming below detection limit)\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtype == 'object':\n",
    "                self.df[col] = self.df[col].astype(str).str.replace(',', '.')\n",
    "                self.df[col] = self.df[col].replace('/', 0)\n",
    "\n",
    "                # Convert to numeric if possible (ignore 'Type_event' for now)\n",
    "                if col != 'Type_event':\n",
    "                    self.df[col] = pd.to_numeric(self.df[col], errors='coerce').fillna(0)\n",
    "\n",
    "        # Encode Categorical Variables (Rain/Snow)\n",
    "        self.df['is_rain'] = (self.df['Type_event'] == 'rain').astype(int)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def engineer_features(self):\n",
    "        \"\"\"\n",
    "        Crucial Step: Weathering is cumulative.\n",
    "        We must calculate the total exposure the rock has had up to this timestep.\n",
    "        \"\"\"\n",
    "        # Sort by Rock and Time to ensure cumulative sums are correct\n",
    "        self.df = self.df.sort_values(by=['Rock_number', 'Timestep'])\n",
    "\n",
    "        # Group by Rock Number\n",
    "        g = self.df.groupby('Rock_number')\n",
    "\n",
    "        # 1. Cumulative Water Exposure\n",
    "        self.df['cum_water'] = g['Event_quantity'].cumsum()\n",
    "\n",
    "        # 2. Cumulative Acid Load (Event Amount * Acid Concentration approximation)\n",
    "        # Assuming 'Acid' column is a concentration factor, if it's pH, logic differs slightly.\n",
    "        # Here we treat it as a load factor.\n",
    "        self.df['cum_acid_load'] = g.apply(lambda x: (x['Event_quantity'] * x['Acid']).cumsum()).reset_index(level=0, drop=True)\n",
    "\n",
    "        # 3. Time Variance (Lag features - what happened in the previous step?)\n",
    "        # Shift targets to use previous leaching as input for current leaching\n",
    "        # (Optional: Can improve accuracy but requires sequential prediction in deployment)\n",
    "        # for target in self.targets:\n",
    "        #     self.df[f'prev_{target}'] = g[target].shift(1).fillna(0)\n",
    "\n",
    "        return self.df"
   ],
   "id": "6672574afdfb2131",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T12:54:51.076204Z",
     "start_time": "2025-12-07T12:54:51.068943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "class LeachatePredictor:\n",
    "    def __init__(self, df, feature_cols, target_cols):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_cols = target_cols\n",
    "        # Changed: We now store a dictionary of models, one per target\n",
    "        self.models = {}\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X = self.df[self.feature_cols]\n",
    "        y = self.df[self.target_cols]\n",
    "\n",
    "        # Random split (or GroupShuffleSplit if using Rock_number)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        print(f\"Training separate models for {len(self.target_cols)} targets...\")\n",
    "\n",
    "        # Iterate through each specific leachate target\n",
    "        for target in self.target_cols:\n",
    "            # 1. Create a mask where the value is NOT 0\n",
    "            # (Assuming 0 implies non-detect/missing)\n",
    "            mask = y_train[target] != 0\n",
    "\n",
    "            # 2. Filter X and y to exclude 0s for this specific target\n",
    "            X_filtered = X_train.loc[mask]\n",
    "            y_filtered = y_train.loc[mask, target]\n",
    "\n",
    "            if len(X_filtered) < 10:\n",
    "                print(f\"  Skipping {target}: Not enough non-zero samples ({len(X_filtered)})\")\n",
    "                continue\n",
    "\n",
    "            # 3. Define the Estimator\n",
    "            model = xgb.XGBRegressor(\n",
    "                booster=\"dart\",\n",
    "                objective='reg:squarederror',\n",
    "                colsample_bytree=0.7,\n",
    "                subsample=0.8,\n",
    "                gamma=0,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1,\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            # 4. Fit only on non-zero data\n",
    "            model.fit(X_filtered, y_filtered)\n",
    "\n",
    "            # 5. Store the model\n",
    "            self.models[target] = model\n",
    "\n",
    "        print(\"Training Complete.\")\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        results = {}\n",
    "        # We need a place to store predictions aligned with the original dataframe shape\n",
    "        # Initialize with 0s\n",
    "        all_preds_df = pd.DataFrame(0.0, index=X_test.index, columns=self.target_cols)\n",
    "\n",
    "        for target in self.target_cols:\n",
    "            if target not in self.models:\n",
    "                continue # Skip if we didn't train a model for this (e.g. all 0s)\n",
    "\n",
    "            model = self.models[target]\n",
    "\n",
    "            # Generate predictions for ALL test rows (the model can predict non-zero even if actual is 0)\n",
    "            preds = model.predict(X_test)\n",
    "            all_preds_df[target] = preds\n",
    "\n",
    "            # --- EVALUATION LOGIC ---\n",
    "            # Mask the TEST set: Only score against actual non-zero values\n",
    "            mask = y_test[target] != 0\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                results[target] = {'R2': np.nan, 'RMSE': np.nan}\n",
    "                continue\n",
    "\n",
    "            # Filter actuals and predictions using the mask\n",
    "            y_true_filtered = y_test.loc[mask, target]\n",
    "            y_pred_filtered = preds[mask] # preds is a numpy array, boolean indexing works if shape matches\n",
    "\n",
    "            # Calculate metrics on non-zero subset\n",
    "            r2 = r2_score(y_true_filtered, y_pred_filtered)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_filtered, y_pred_filtered))\n",
    "\n",
    "            results[target] = {'R2': r2, 'RMSE': rmse}\n",
    "\n",
    "        return results, all_preds_df"
   ],
   "id": "1d0f7741d90ad196",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T13:00:01.235010Z",
     "start_time": "2025-12-07T13:00:01.194777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_results(y_test, preds_df, target_name):\n",
    "    # 1. Filter Data: Only plot points where Actual value is NOT 0\n",
    "    # This matches the training logic (ignoring non-detects)\n",
    "    mask = y_test[target_name] != 0\n",
    "\n",
    "    if mask.sum() < 5:\n",
    "        print(f\"Skipping plot for {target_name}: Not enough non-zero data points.\")\n",
    "        return\n",
    "\n",
    "    actuals = y_test.loc[mask, target_name]\n",
    "    predictions = preds_df.loc[mask, target_name]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(actuals, predictions, alpha=0.6, color='blue')\n",
    "\n",
    "    # Perfect prediction line\n",
    "    min_val = min(actuals.min(), predictions.min())\n",
    "    max_val = max(actuals.max(), predictions.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "    plt.title(f'Actual vs Predicted: {target_name} (Non-Zero Only)')\n",
    "    plt.xlabel('Actual Concentration/Value')\n",
    "    plt.ylabel('Predicted Concentration/Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# --- 1. Plot Prediction Scatter Plots ---\n",
    "# 'predictions' variable comes from: metrics, predictions = predictor.evaluate(X_test, y_test)\n",
    "targets_to_plot = predictor.target_cols\n",
    "\n",
    "for t in targets_to_plot:\n",
    "    if t in y_test.columns and t in predictions.columns:\n",
    "        plot_results(y_test, predictions, t)\n",
    "\n",
    "# --- 2. Plot Feature Importance ---\n",
    "# We now have multiple models (one per target).\n",
    "# Let's plot importance for the first available target as an example.\n",
    "\n",
    "if len(predictor.models) > 0:\n",
    "    # Get the first target name and its corresponding model\n",
    "    first_target_name = list(predictor.models.keys())[0]\n",
    "    first_model = predictor.models[first_target_name]\n",
    "\n",
    "    # Extract importances\n",
    "    importances = pd.Series(first_model.feature_importances_, index=input_features).sort_values(ascending=False).head(10)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    importances.plot(kind='barh', color='teal')\n",
    "    plt.title(f'Top 10 Feature Importance for {first_target_name}')\n",
    "    plt.gca().invert_yaxis() # Highest importance at top\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models were trained successfully.\")"
   ],
   "id": "a12b193e9590d12e",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.ensemble._hist_gradient_boosting.predictor' has no attribute 'target_cols'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 33\u001B[39m\n\u001B[32m     29\u001B[39m     plt.show()\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# --- 1. Plot Prediction Scatter Plots ---\u001B[39;00m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# 'predictions' variable comes from: metrics, predictions = predictor.evaluate(X_test, y_test)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m targets_to_plot = \u001B[43mpredictor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtarget_cols\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m targets_to_plot:\n\u001B[32m     36\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m y_test.columns \u001B[38;5;129;01mand\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m predictions.columns:\n",
      "\u001B[31mAttributeError\u001B[39m: module 'sklearn.ensemble._hist_gradient_boosting.predictor' has no attribute 'target_cols'"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T12:56:41.593299888Z",
     "start_time": "2025-12-07T12:47:40.428743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the trained model for future use\n",
    "import joblib\n",
    "joblib.dump(predictor.model, 'leachate_model.pkl')\n",
    "print(\"Trained model saved as 'leachate_model.pkl'.\")"
   ],
   "id": "da8c83412542e450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved as 'leachate_model.pkl'.\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
